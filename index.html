<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Lei Wang</title>
    <meta content="Lei Wang, https://gudaochangsheng.github.io" name="keywords">
    <style media="screen" type="text/css">
        html,
        body,
        div,
        span,
        applet,
        object,
        iframe,
        h1,
        h2,
        h3,
        h4,
        h5,
        h6,
        p,
        blockquote,
        pre,
        a,
        abbr,
        acronym,
        address,
        big,
        cite,
        code,
        del,
        dfn,
        em,
        font,
        img,
        ins,
        kbd,
        q,
        s,
        samp,
        small,
        strike,
        strong,
        sub,
        tt,
        var,
        dl,
        dt,
        dd,
        ol,
        ul,
        li,
        fieldset,
        form,
        label,
        legend,
        table,
        caption,
        tbody,
        tfoot,
        thead,
        tr,
        th,
        td {
            border: 0pt none;
            font-family: inherit;
            font-size: 100%;
            font-style: inherit;
            font-weight: inherit;
            margin: 0pt;
            outline-color: invert;
            outline-style: none;
            outline-width: 0pt;
            padding: 0pt;
            vertical-align: baseline;
        }

        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        a.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        b.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        * {
            margin: 0pt;
            padding: 0pt;
        }

        body {
            position: relative;
            margin: 3em auto 2em auto;
            width: 800px;
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 14px;
            background: #eee;
        }

        h2 {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 15pt;
            font-weight: 700;
        }

        h3 {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 16px;
            font-weight: 700;
        }

        strong {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 13px;
            font-weight: bold;
        }

        ul {
            list-style: circle;
        }

        img {
            border: none;
        }

        li {
            padding-bottom: 0.5em;
            margin-left: 1.4em;
        }

        alert {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 13px;
            font-weight: bold;
            color: #FF0000;
        }

        em,
        i {
            font-style: italic;
        }

        div.section {
            clear: both;
            margin-bottom: 1.5em;
            background: #eee;
        }

        div.spanner {
            clear: both;
        }

        div.paper {
            clear: both;
            margin-top: 0.5em;
            margin-bottom: 1em;
            border: 1px solid #ddd;
            background: #fff;
            padding: 1em 1em 1em 1em;
        }

        div.paper div {
            padding-left: 230px;
        }

        img.paper {
            margin-bottom: 0.5em;
            float: left;
            width: 200px;
        }

        span.blurb {
            font-style: italic;
            display: block;
            margin-top: 0.75em;
            margin-bottom: 0.5em;
        }

        pre,
        code {
            font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
            margin: 1em 0;
            padding: 0;
        }

        div.paper pre {
            font-size: 0.9em;
        }
    </style>

    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet"
          type="text/css"/>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164510176-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-164510176-1');
    </script>

</head>


<body>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 130px; display: flex; align-items: center;">
    <div style="flex: 1; display: flex; align-items: center;">
        <img title="implus" style="height: 130px;" src="./resources/images/me.jpg">
        <div style="padding-left: 1em;">
            <span style="line-height: 150%; font-size: 20pt;">Lei Wang (王雷)</span><br>
            <span>Ph. D. Candidate, College of Computer Science, Nankai University</span><br>
            <span><strong>Address</strong>: No. 38, Tongyan Road, Haihe Education Park, Tianjin, China</span><br>
            <span><strong>Email</strong>: scitop1998 [at] {gmail.com}</span><br>
            <!-- <span><strong>Research Group Page</strong>: <a href="https://github.com/IMPlus-PCALab">IMPlus@PCALab</a></span><br> -->
        </div>
    </div>
    <div style="flex: 0; padding-left: 1em;">
        <img title="Nankai University" style="height: 120px;" src="./resources/images/nankai_logo.png">
    </div>
</div>

<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<div style="clear: both;">
    <div class="section">
        <h2>About Me [<a href="https://github.com/gudaochangsheng">GitHub</a>]
            [<a href="https://scholar.google.com/citations?user=6Z66DAwAAAAJ&hl=zh-CN">Google Scholar</a>]
	    [<a href="https://gudaochangsheng.blog.csdn.net/">CSDN</a>]
	    [<a href="https://www.zhihu.com/people/gudaochangsheng">知乎</a>]
            <!--[<a href="./resources/cv/wwh_cv.pdf">CV</a>])-->
        </h2>
        <div class="paper">
            I'm a Ph. D. candidate in PCA Lab, VCIP, College of Computer Science, Nankai University. 
            I got my master's degree from the College of Information Science and Technology, Hebei Agricultural University (HEBAU) in 2024. My master's advisor is <a href="https://xinxi.hebau.edu.cn/info/1050/1350.htm">Prof. Bo Liu</a> from HEBAU.           
	    My PhD's advisor is <a href="https://gsmis.njust.edu.cn/open/TutorInfo.aspx?dsbh=t6AONNl4pZ5la8fwtaQrXw==&yxsh=z70ppxVSQAs=&zydm=eNM2K21sgHA=">Prof. Jian Yang</a> from Nankai University (NKU), who is a Changjiang Scholar. My vice-advisor is <a href="https://yaxingwang.github.io/">Prof. Yaxing Wang</a> from NKU.
<!-- 	    I started my postdoctoral career in NJUST as a candidate for the <a href="https://zhuanlan.zhihu.com/p/147471409">2020 Postdoctoral Innovative Talent Program</a>, supervised by <a href="https://imag-njust.net/jinhui-tang/">Prof. Jinhui Tang</a>.
	    In 2016, I spent 8 months as a research intern in Microsoft Research Asia, supervised by <a href="https://scholar.google.com/citations?user=Bl4SRU0AAAAJ&hl=zh-CN">Prof. Tao Qin</a> and <a href="https://scholar.google.com/citations?user=Nh832fgAAAAJ&hl=zh-CN">Prof. Tie-Yan Liu</a>.
            I was a visiting scholar at <a href="https://www.momenta.cn/">Momenta</a>, mainly focusing on monocular perception algorithm. -->
            <br><br>

            My recent works are mainly on:
	    <ul>
		<li>Unified Multimodal Understanding and Generation</li>
		<li>diffusion model</li>
		<li>Vision-Language Model</li>
		<li>knowledge distillation</li>
		<li>gait recognition</li>
<!-- 		<li>low-level vision</li> -->
<!-- 		<li>unsupervised learning</li>
		<li>knowledge distillation</li> -->
	    </ul>
<!--             <p style='color:red'><strong>
                 We are looking for self-motivated PhD candidates! Please feel free to contact me through the email (attach your CV, at least with a CCF-A paper).
		 During the PhD career, you can have:
		 <ul> 
			<li>joint supervision with well-known research institute (e.g., Megvii, SenseTime Research, Huawei Noah's Ark Lab, BAAI)</li>
			<li>hand in hand guidance to publish earlier papers</li>
			<li>relatively flexible and free research space</li>
		 </ul>
		 We would not push hard, but you should always be self-driven for your own target, i.e., making solid and impactful contributions to the CV/AI community.
            </strong></p>  -->
        </div>
    </div>
</div>


<div style="clear: both;">
    <div class="section">
        <h2 id="confpapers">Honor</h2>
        <div class="paper">
            <ul>
		<li>
		    2021-2022 Postgraduate School First Class Scholarships
		</li>
		<li>
                    National Bronze Medal in the "China University Computer Competition - Group Programming Ladder Match" in Hebei Province in 2019.
                </li>
<!--                 <li>
                    Second place of 2020 Zhengtu Cup's first AI competition, namely the industrial defect detection algorithm, <strong>150,000 RMB bonus (2nd from 900 teams)</strong>
                </li> -->
                <li>
                    Second Prize of C/C++ Programming University B Group of the 10th Blue Bridge Cup National Software and Information Technology Professionals Competition Hebei Region
                </li>
                <li>
                    2018 Third Prize in the Professional Group of the Second College Student Programming Competition of Hebei Agricultural University
                </li>
<!--                 <li>
                    2015 Dean Medal of School of Computer Science, Nanjing University of Science and Technology, 2016 Presidential Medal of Nanjing University of Science and Technology, 2016 National Scholarship
                </li>
                <li>
                    ACM-ICPC Asia Regional Contest, Silver Medal (1st)
                </li> -->
            </ul>
            <div class="spanner"></div>
        </div>
    </div>
</div>

<div style="clear: both;">
    <div class="section">
        <h2 id="experience">News</h2>
        <div class="paper">
            <ul>
		<li>
			2025-2-27: 2 papers (<a href="https://arxiv.org/pdf/2505.03097">MaskUNet</a>, <a href="https://drive.google.com/file/d/1CenAPN9qPvBhDCc0Za6Fx6yXGXZCMpst/view">TiUE</a>) accepted in CVPR 2025.
		</li>
		<li>
			2023-12-08: I was accepted as a <a href="https://cc.nankai.edu.cn/2023/1208/c13297a531077/page.htm">PhD candidate</a> in Computer Science and Technology at the College of Computer Science, Nankai University.
		</li>
		<li>
			2023-11-17: 1 paper (<a href="https://arxiv.org/pdf/2311.11210.pdf">HiH</a>) submitted in Arxiv 2024. 
		</li>
		<li>
			2023-07-14: 1 paper (<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Hierarchical_Spatio-Temporal_Representation_Learning_for_Gait_Recognition_ICCV_2023_paper.pdf">HSTL</a>) accepted in ICCV 2023. 
		</li>
		<li>
			2023-07-1: 1 paper (<a href="https://ieeexplore.ieee.org/abstract/document/10221893/">GaitMM</a>) accepted in ICIP 2023. 
		</li>
		<li>
			2022-04-4: 1 paper (<a href="https://kns.cnki.net/kcms2/article/abstract?v=SDjqx_HoHgu5pJiEhLEUbVshYNbTCYZdcIvhRqU01zXmyHraW6JQR4uUxMtEfiozr-47QhMZ6GBXguNzrkKnI_fvbLSlW64wFC1gSSkj5aw294FLZQoYGvM2qHhxyQzDJi1rG52PbLBqmkNMTh_3aw==&uniplatform=NZKPT&language=CHS">GaitTB</a>) accepted in 火力与指挥控制. 
		</li>
            </ul>
            <div class="spanner"></div>
        </div>
    </div>
</div>
    


<div style="clear: both;">
    <div class="section">
        <h2 id="confpapers">Publications</h2>
	(* indicates equal contribution, # corresponding author)

	<div class="paper"><img class="paper" src="./resources/paper_icon/maskunet.png"
				   title="Not All Parameters Matter: Masking Diffusion Models for Enhancing Generation Ability">
            <div><strong>Not All Parameters Matter: Masking Diffusion Models for Enhancing Generation Ability</strong><br>
		<strong>Lei Wang</strong>,  Senmao Li, Fei Yang#, Jianye Wang, Ziheng Zhang, Yuhan Liu, Yaxing Wang,  Jian Yang#<br>
                in CVPR, 2025<br>
                <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Not_All_Parameters_Matter_Masking_Diffusion_Models_for_Enhancing_Generation_CVPR_2025_paper.pdf">[Paper]</a>
                <a href="./resources/bibtex/CVPR25-maskunet.txt">[BibTex]</a>
		<a href="https://gudaochangsheng.github.io/MaskUnet-Page/">[Project Page]</a>
                <a href="https://github.com/gudaochangsheng/MaskUnet">[Code]</a><img
                        src="https://img.shields.io/github/stars/gudaochangsheng/MaskUnet?style=social"/>
                <br>
<!--                 <a href="https://paperswithcode.com/sota/object-detection-in-aerial-images-on-dota-1?p=large-selective-kernel-network-for-remote"><img
                src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/large-selective-kernel-network-for-remote/object-detection-in-aerial-images-on-dota-1"/></a>
                <a href="https://paperswithcode.com/sota/object-detection-in-aerial-images-on-hrsc2016?p=large-selective-kernel-network-for-remote"><img
                src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/large-selective-kernel-network-for-remote/object-detection-in-aerial-images-on-hrsc2016"/></a>
                <br> -->
                <alert>
        In this paper, we propose a universal masking strategy, MaskUNet, to improve the generation quality and generalization ability of stable diffusion. 
		</alert>
            </div>
            <div class="spanner"></div>
        </div>

	<div class="paper"><img class="paper" src="./resources/paper_icon/Loopfree.jpg"
				   title="One-Way Ticket : Time-Independent Unified Encoder for Distilling Text-to-Image Diffusion Model">
            <div><strong>One-Way Ticket : Time-Independent Unified Encoder for Distilling Text-to-Image Diffusion Model</strong><br>
		Senmao Li, <strong>Lei Wang</strong>, Kai Wang, Tao Liu, Jiehang Xie, Joost van de Weijier, Fahad Shahbaz Khan, Shiqi Yang, Yaxing Wang#, Jian Yang<br>
                in CVPR, 2025<br>
                <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_One-Way_Ticket_Time-Independent_Unified_Encoder_for_Distilling_Text-to-Image_Diffusion_Models_CVPR_2025_paper.pdf">[Paper]</a>
		<a href="./resources/bibtex/cvpr2025_tiue.txt">[BibTex]</a>
                <a href="https://github.com/sen-mao/Loopfree">[Code]</a><img
                        src="https://img.shields.io/github/stars/sen-mao/Loopfree?style=social"/>
                <br>
<!--                 <a href="https://paperswithcode.com/sota/object-detection-in-aerial-images-on-dota-1?p=large-selective-kernel-network-for-remote"><img
                src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/large-selective-kernel-network-for-remote/object-detection-in-aerial-images-on-dota-1"/></a>
                <a href="https://paperswithcode.com/sota/object-detection-in-aerial-images-on-hrsc2016?p=large-selective-kernel-network-for-remote"><img
                src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/large-selective-kernel-network-for-remote/object-detection-in-aerial-images-on-hrsc2016"/></a>
                <br> -->
                <alert>
        We introduce the first Time-independent Unified Encoder (TiUE) architecture, which is a loop-free distillation approach and eliminates the need for iterative noisy latent processing while maintaining high sampling fidelity with a time cost comparable to previous one-step methods.
		</alert>
            </div>
            <div class="spanner"></div>
        </div>

	<div class="paper"><img class="paper" src="./resources/paper_icon/ICCV23_HSTL.jpg"
				   title="Hierarchical Spatio-Temporal Representation Learning for Gait Recognition">
            <div><strong>Hierarchical Spatio-Temporal Representation Learning for Gait Recognition</strong><br>
		<strong>Lei Wang</strong>, Bo Liu#, Fangfang Liang, Bincheng Wang<br>
                in ICCV, 2023<br>
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Hierarchical_Spatio-Temporal_Representation_Learning_for_Gait_Recognition_ICCV_2023_paper.pdf">[Paper]</a>
                <a href="./resources/bibtex/ICCV_2023_HSTL.txt">[BibTex]</a>
                <a href="https://github.com/gudaochangsheng/HSTL">[Code]</a><img
                        src="https://img.shields.io/github/stars/gudaochangsheng/HSTL?style=social"/>
                <br>
<!--                 <a href="https://paperswithcode.com/sota/object-detection-in-aerial-images-on-dota-1?p=large-selective-kernel-network-for-remote"><img
                src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/large-selective-kernel-network-for-remote/object-detection-in-aerial-images-on-dota-1"/></a>
                <a href="https://paperswithcode.com/sota/object-detection-in-aerial-images-on-hrsc2016?p=large-selective-kernel-network-for-remote"><img
                src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/large-selective-kernel-network-for-remote/object-detection-in-aerial-images-on-hrsc2016"/></a>
                <br> -->
                <alert>
        In this paper, we propose a hierarchical spatio-temporal representation learning (HSTL) framework for extracting gait features from coarse to fine. 
		</alert>
            </div>
            <div class="spanner"></div>
        </div>

	<div class="paper"><img class="paper" src="./resources/paper_icon/ICIP23_GaitMM.jpg"
				   title="GaitMM: Multi-Granularity Motion Sequence Learning for Gait Recognition">
            <div><strong>GaitMM: Multi-Granularity Motion Sequence Learning for Gait Recognition</strong><br>
		<strong>Lei Wang</strong>, Bo Liu#, Bincheng Wang, Fuqiang Yu<br>
                in ICIP, 2023<br>
                <a href="https://ieeexplore.ieee.org/abstract/document/10221893">[Paper]</a>
		<a href="https://arxiv.org/pdf/2209.08470.pdf">[Arxiv]</a>
                <a href="./resources/bibtex/ICCV_2023_HSTL.txt">[BibTex]</a>
                <a href="https://github.com/gudaochangsheng/ourcode">[Code]</a><img
                        src="https://img.shields.io/github/stars/gudaochangsheng/ourcode?style=social"/>
                <br>
                <alert>
        In this study, we propose a multi-granularity motion representation network (GaitMM) for gait sequence learning.
		</alert>
            </div>
            <div class="spanner"></div>
        </div>

	<div class="paper"><img class="paper" src="./resources/paper_icon/Arxiv24_HiH.jpg"
				   title="HiH: A Multi-modal Hierarchy in Hierarchy Network for Unconstrained Gait Recognition">
            <div><strong>HiH: A Multi-modal Hierarchy in Hierarchy Network for Unconstrained Gait Recognition</strong><br>
		<strong>Lei Wang</strong>, Yinchi Ma, Peng Luan, Wei Yao, Congcong Li, Bo Liu#<br>
                in Arxiv, 2024<br>
                <a href="https://arxiv.org/pdf/2311.11210.pdf">[Paper]</a>
                <a href="./resources/bibtex/Arxiv_2024_HiH.txt">[BibTex]</a>
                <a href="">[Code]</a><img
                        src=""/>
                <br>
                <alert>
       We present a multi-modal Hierarchy in Hierarchy network (HiH) that integrates silhouette and pose sequences for robust gait recognition.
		</alert>
            </div>
            <div class="spanner"></div>
        </div>

    </div>
</div>

<div style="clear: both;">
    <div class="section">
        <h2 id="confpapers">Internships</h2>
        <div class="paper">
            <ul>
		<li>
		    2025.08 - present, Visual Technology Department, Baidu, Beijing, China.  Mentor: <a href="https://scholar.google.com/citations?user=1uL_9HAAAAAJ&hl=zh-CN&oi=ao">Yuxin Song</a>
		</li>
            </ul>
            <div class="spanner"></div>
        </div>
    </div>
</div>


<div style="clear: both;">
    <div class="section">
        <h2>Review Services</h2>
        <div class="paper">
<!--             <strong>Journal Reviewer</strong><br>
	    IEEE Transactions on Neural Networks and Learning Systems (TNNLS)<br>
	    IEEE Transactions on Image Processing (TIP)<br>
            IEEE Transactions on Multimedia (TMM)<br>
	    International Journal of Computer Vision (IJCV)<br>
	    IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)<br>
            <br>
 -->
            <strong>Conference Reviewer</strong><br>
	    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024<br>
	    Asian Conference on Computer Vision, 2022<br>
        </div>
    </div>
</div>

<div style="clear: both;">
    <div class="section">
        <h2>Friendly Links</h2>
        <div class="paper">
	    <ul>
		<li>
			 <a href="https://sen-mao.github.io/"><strong>Senmao Li (李森茂)</strong></a>
		</li>
		<li>
			 <a href="https://xiantaohu.github.io/"><strong>Xiantao Hu (胡现韬)</strong></a>
		</li>
		<li>
			 <a href="https://montaellis.github.io/"><strong>Kangneng Zhou (周康能)</strong></a>
		</li>
<!-- 		<li>
			2023-11-17: 1 paper (<a href="https://arxiv.org/pdf/2311.11210.pdf">HiH</a>) submitted in Arxiv 2024. 
		</li>
		<li>
			2023-07-14: 1 paper (<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Hierarchical_Spatio-Temporal_Representation_Learning_for_Gait_Recognition_ICCV_2023_paper.pdf">HSTL</a>) accepted in ICCV 2023. 
		</li>
		<li>
			2023-07-1: 1 paper (<a href="https://ieeexplore.ieee.org/abstract/document/10221893/">GaitMM</a>) accepted in ICIP 2023. 
		</li>
		<li>
			2022-04-4: 1 paper (<a href="https://kns.cnki.net/kcms2/article/abstract?v=SDjqx_HoHgu5pJiEhLEUbVshYNbTCYZdcIvhRqU01zXmyHraW6JQR4uUxMtEfiozr-47QhMZ6GBXguNzrkKnI_fvbLSlW64wFC1gSSkj5aw294FLZQoYGvM2qHhxyQzDJi1rG52PbLBqmkNMTh_3aw==&uniplatform=NZKPT&language=CHS">GaitTB</a>) accepted in 火力与指挥控制. 
		</li> -->
            </ul>
        </div>
    </div>
</div>

<div style='width:600px;height:300px;margin:0 auto'>
    <!--<a href="https://clustrmaps.com/site/1b7cl" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=wHZzsZcsBCB6HxOqmaDqDJX5oLT_yvrQ5HQx9agcvJo&cl=ffffff"></a>-->
    <!--<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=wHZzsZcsBCB6HxOqmaDqDJX5oLT_yvrQ5HQx9agcvJo&cl=ffffff&w=a"></script>-->
    <!--<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=ItPniLpvAlUyBKssdgFtKwBDg8lP1ao3ju4dmDzw6uA&cl=ffffff&w=a"></script>-->
<!--     <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=ItPniLpvAlUyBKssdgFtKwBDg8lP1ao3ju4dmDzw6uA&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script> -->
	<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=aV13XTfCXGdOSgImNKS632UK6iV7YbDHWySo_hIWg0g&cl=ffffff&w=a"></script>
</div>

</body>
</html>
